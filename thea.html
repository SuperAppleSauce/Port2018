<!DOCTYPE html>
<html lang="en">

<head>

  <title>Alexis Trevizo</title>
  <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width">

  <link rel="stylesheet" href="path/to/font-awesome/css/font-awesome.min.css">

  <link rel="stylesheet" type="text/css" href="stylesProject.css">

</head>
<header>

  <a id="brand" href="http://alexistrevizo.com/">
    Alexis Trevizo
  </a>


</header>

<body>
  <button id="section1Btn">
    back to top
    </svg>


  </button>

  <section class="projectHero">

    <h1>Thea</h1>
    <div class="projectOverview">
      <p class="projectQuote">A 5G and AI connectivity solution that allows the blind, low vision, (BLV) community to
        navigate throughout New York City through haptic feedback.</p>
      <div class="projectIngredients">



        <div class="collabs">
          <p class="smallBrow">My role</p>
          <p class="smallBrowContent">Designer, researcher, & prototyper</p>
        </div>
        <div class="collabs">
          <p class="smallBrow">Contributors</p>
          <p class="smallBrowContent">Darshan Alatar Patel, Lauren Fox, Alina Peng and Chanel Luu Hai</p>
        </div>
        <p class="projectDate collabs">June 2018</p>
      </div>
    </div>





    <div class="mainImage" style="padding:56.25% 0 0 0;position:relative;"><iframe
        src="https://player.vimeo.com/video/282612171" style="position:absolute;top:0;left:0;width:100%;height:100%;"
        frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></div>
    <script src="https://player.vimeo.com/api/player.js"></script>


    <div class="scrollBoi"></div>
  </section>

  <section id="myP1" class="projectDetails">

    <!--     How might we section
       -->
    <article class="projectPhase">
      <h2 class="phaseTitle">How might we...</h2>
      <div class="phaseDetails">
        <div class="howMight">
          <p>How might we improve the experience of individuals with mobility challenges in NYC?</p>
          <p>How might we create a solution that BLV people better navigate in an indoor space?
          </p>
          <p>How might we take the stress out of pre-planning for BLV people?
          </p>
          <p>How might we create a technology that uses sensory inputs and tactile feedback to deliver navigation
            information?
          </p>
          <p>How might we better inform sighted passersby of a disability? How might we raise awareness for BLV
            community as a whole?</p>
        </div>
    </article>

    <!-- DiscoveryPhase -->
    <article class="projectPhase">
      <h2 class="phaseTitle">Discovery</h2>
      <div class="phaseDetails">
        <!-- <h3>THE OPPORTUNITY</h3> -->
        <p>We conducted research by interviewing 43+ BLV users and 20+ subject matter experts, including people from
          Verizon’s 5G innovation Lab, Verizon’s accessibility group, and disability volunteer coordinators. We
          conducted empathy experiments to increase our empathy for the community </p>
      </div>

      <img
        src="https://uc2e1d994ce98a0c980fb3181c59.dl.dropboxusercontent.com/cd/0/inline/A9yPsj14b3XclkUigSz53ouvdaRPt_6e5bg7vxya_2QWVK3-cN_ch_WqJtnnP9hRiuvSfvrY1hPTQ0kLLOkecIZryrPn2sRRI3SjwhwS13fQejJgDtoSwwRHL6ow2ICvqFk/file#"
        class="artifactImage">
      <div class="spacer"></div>
      <div class="phaseDetails">

        <h3>Early Findings</h3>

        <ol>
          <li>People who are congenitally blind (blind from birth) and those who have been dealing with vision
            impairment for a long time are more in tune with their other senses than sighted individuals.
          </li>
          <li>People who are visually impaired intensively plan their trips — both short- and long-term — to ease
            navigation and avoid obstacles that prohibit them from getting around.</li>
          <li>Sight canes and other wearable products (e.g. glasses) are essential tools for the BLV community, however,
            they have also evolved into symbols that signal one’s impairment to a pedestrian.</li>
          <li>Current way-finding technologies do a decent job of getting a user from one address to another but leave
            them stranded once they pass through the door. That “last mile,” such as finding a specific room, aisle, or
            product, poses major difficulties for a blind or low-vision person.</li>
        </ol>

      </div>

    </article>

    <!-- User info area 
      <article class="projectPhase">
        <h2>The Users</h2>
        <div class="phaseDetails projectPersonas">
  
          <p class="userDetails">Beatriz, 34, Wholesaler</p>
          <p>{{User details}}</p>
          <div class="userQuote">
     
            <p>{{User quote}}</p>
          </div>
  
        </div>
  
        <div class="spacer"></div>
        <div class="phaseDetails">
          <h3>MOTIVATIONS</h3>
          <ol>
            <li>MG#</li>
            <li>cool</li>
            <li>cool</li>
            <li>cool</li>
          </ol>
        </div>
  
      </article>  -->


    <!-- research  area -->
    <article class="projectPhase">
      <h2>Diving Deeper</h2>
      <div class="phaseDetails">

        <p>To develop our ideas, we purposefully chose to co-create with our users. We hosted a workshop at a local
          library with eleven BLV users. As empathetic as we tried to be, we knew we could never truly know what it was
          like to be our users. We asked our participants to focus on the following problem:</p>

        <div class="userQuote">
          <p>How might we improve your daily commute?</p>
        </div>
      </div>

      <div class="spacer"></div>
      <div class="phaseDetails">
        <h3>findings</h3>
        <p>Using clay and legos, we built solutions and shared them with the group. A few of the "winning" solutions
          were:</p>
        <ol>
          <li>Responsive sidewalks that vibrate to let pedestrians know to get out of the way</li>
          <li>A wand that can detect and describe different types of light to a user</li>
          <li>Haptic shoes</li>
        </ol>
        <p>Incorporating our findings, we knew we needed haptic feedback to direct BLV users. We finally decided on a
          haptic feedback pad that could be easily worn anywhere on the body. </p>
      </div>

      <img
        src="https://uc9cc4c3b03b438f762f06bb77a6.dl.dropboxusercontent.com/cd/0/inline/A9xHf1igXzuxEFTQiuba2E4bsDsyaueoNEdJcUU5fFlEEcayjQTQqQylrnu6Ar72FWFtNrN6zLfHam3yWHEDMha2tfHJ3GtcqTyjW1BrUZSPWmSq69WOcUCpOqfo3VRCzBw/file#"
        class="artifactImage">
    </article>

    <!--     Testing + validation -->
    <article class="projectPhase">
      <h2>Testing and Validation</h2>
      <div class="phaseDetails">
        <p>To test our whether our idea was viable, we created a series of low fidelity Arduino prototypes. Darshan and
          I led the team in the Arduino development by guiding and educating other members of the team on how to use
          simple wiring and circuitry to create haptic feedback in our prototype. We tested the pad on various users and
          found the following insights:</p>
        <img
          src="https://uc603e1157d6d61f872d91e71509.dl.dropboxusercontent.com/cd/0/inline/A9zMtkYBnx7KOlc8RctMuyYiqlOrfJvSTCgiIX2yUT-ESu_inPq7PUchRvU0LvcYAubhOBjqq7tJZQU8nMv90pAU8xbMCmUXrJEWz1N_5YEEG9RN9-2NvLblTqO8gGpdHyw/file#"
          class="artifactImage">

      </div>

      <div class="spacer"></div>
      <div class="phaseDetails">
        <h3>new insights</h3>
        <ol>
          <li>The preferred location for the pad was the shoulders because it was the easiest for users to receive
            feedback when navigating.</li>
          <li>A series of pulses was a good indicator for a user to continue going straight, and a faster-paced pulse
            would tell the user to stop. </li>
          <li>The pad would ultimately help direct BLV users in their daily commute.</li>

        </ol>
      </div>

    </article>
    <!--     the solution -->
    <article class="projectPhase">
      <h2>The Solution</h2>
      <div class="phaseDetails">
        <p>Prior to venturing out of the house, people with vision impairments have to extensively pre-plan, which
          stifles spontaneity. On the way, they could encounter obstacles in their path—like major train delays or road
          construction—that derail their plans. These types of problems are typically easy for a sighted person to deal
          with, but for those with a visual impairment, they require immediate, up-to-date information that provides
          options for re-routing. The “last foot” in navigation is a serious challenge. For example, getting to a subway
          station is possible, but finding the correct platform proves difficult, especially if trains change.
        </p>
        <br>
        <P>As a result, we created Thea, a concept that offers a comprehensive solution to help the visually impaired
          independently and safely explore their environments.</p>

        <img src="http://www.momentdesign.com/wp-content/uploads/2018/08/Thea_CaseStudy_Speech-07.jpg"
          class="artifactImage">

        <h3>Natural and conversational UI</h3>
        <P>Thea is a system that is easy to use and understand. It responds to requests like a real person, adapts to
          voice inputs, and provides an unparalleled navigation experience.</P>
        <img src="http://www.momentdesign.com/wp-content/uploads/2018/08/thea-haptic-feedback-768x515.jpg"
          class="artifactImage">

        <h3>Intuitive haptic feedback</h3>
        <p>Visually-impaired individuals rely heavily on their sense of hearing so, in high-congestion and noisy areas,
          Thea switches from audio to haptic feedback. Thea conveys directional information in an intuitive way—its
          haptic “language” orients users and provides complex directional information.</P>

        <img src="http://www.momentdesign.com/wp-content/uploads/2018/08/thea-patch-768x432.jpg" class="artifactImage">

        <h3>Stretchy, wearable pad</h3>
        <P>Thea is a system that is easy to use and understand. It responds to requests like a real person, adapts to
          voice inputs, and provides an unparalleled navigation experience.</P>






      </div>

    </article>

    <!--     final thoughts 
      <article class="projectPhase">
        <h2>Final Thoughts</h2>
        <div class="phaseDetails">
          <p>{{final thoughts + details}}</p>
  
        </div>
  
      </article>  -->

    <!-- hidden new phase area -->
    <!--   <article class="projectPhase">
      <h2>The Users</h2>
      <div class="phaseDetails">
        <h3>the oppertunity</h3>
        <p>Create a digital experience for various members of the life insurances sales process to track applications, increase communication between different players, and enable realtime tracking of performance against sales goals.</p>
      </div>
  
      <img src="https://placekitten.com/g/200/400" class="artifactImageOld">
      
      <div class="phaseDetails column">
        <h3>MOTIVATIONS</h3>
        <ol>
          <li>MG#</li>
          <li>cool</li>
          <li>cool</li>
          <li>cool</li>
        </ol>
  
        <h3>SUCCESS METRICS</h3>
        <div class="successMetrics">
          <p> time</p>
          <p> time</p>
          <p> time</p>
          <p> time</p>
  
        </div>
  
      </div>

              <img src="" class="artifactImage">
      
      <div class="spacer"></div>
      <div class="phaseDetails">
        <h3>the impact</h3>
        <p>mormeomreo mreomreomroemro emroemrome romeormeormeomroemy
  
    </article>
   -->
  </section>


  <script src='https://cdnjs.cloudflare.com/ajax/libs/gsap/3.4.2/gsap.min.js'></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/gsap/3.4.2/ScrollTrigger.min.js'></script>
  <script src='https://s3-us-west-2.amazonaws.com/s.cdpn.io/16327/ScrollToPlugin3.min.js'></script>
  <script src="./script.js"></script>


</body>

</html>
